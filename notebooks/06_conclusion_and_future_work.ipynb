{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c5f4da",
   "metadata": {},
   "source": [
    "<h3 style=\"color: teal; background-color: white; padding: 10px; border-radius: 5px;\"> 06. Conclusion & Future Work </h3> <h4>6.1 Conclusion</h4>\n",
    "\n",
    "We performed a comprehensive analysis of the Bank Marketing dataset, including data exploration, preprocessing, feature engineering, modeling, and evaluation.\n",
    "\n",
    "The target variable is highly imbalanced (majority non-subscribers), which impacts traditional accuracy metrics. Metrics like ROC-AUC, recall, and F1-score provided a more realistic evaluation.\n",
    "\n",
    "Feature engineering improved model interpretability and predictive power:\n",
    "\n",
    "total_contacts, contacted_before, age_group, pdays_transformed were derived to better capture client behavior.\n",
    "\n",
    "Modeling results:\n",
    "\n",
    "Weighted Random Forest achieved the highest ROC-AUC (0.813) and recall (0.638), making it most suitable for identifying potential subscribers.\n",
    "\n",
    "Logistic Regression is highly interpretable with high precision but low recall, useful when minimizing false positives is critical.\n",
    "\n",
    "Decision Tree provides a simpler, interpretable model but is less effective than RF for capturing subscribers.\n",
    "\n",
    "Feature importance analysis revealed that economic indicators (euribor3m, nr.employed, emp.var.rate) and campaign-related features (pdays_transformed, total_contacts) are the most influential predictors.\n",
    "\n",
    "Key Insight:\n",
    "High overall accuracy (>0.85) is misleading due to class imbalance. For marketing campaigns, models that maximize recall (like Weighted RF) are preferred to capture as many potential subscribers as possible, even at the cost of moderate false positives.\n",
    "\n",
    "<h4>6.2 Future Work & Improvements</h4>\n",
    "\n",
    "Class Imbalance Handling\n",
    "\n",
    "Explore additional methods such as SMOTE, ADASYN, or ensemble methods (Balanced Bagging/Boosting) to further improve minority class prediction.\n",
    "\n",
    "Advanced Feature Engineering\n",
    "\n",
    "Incorporate interaction features between economic indicators and campaign variables.\n",
    "\n",
    "Include temporal features (e.g., month, day-of-week effects) in a more sophisticated way, potentially using cyclic encoding.\n",
    "\n",
    "Model Enhancement\n",
    "\n",
    "Experiment with gradient boosting models (XGBoost, LightGBM, CatBoost) which can improve recall and ROC-AUC.\n",
    "\n",
    "Tune hyperparameters using Bayesian optimization for better performance.\n",
    "\n",
    "Threshold Optimization\n",
    "\n",
    "Adjust the classification threshold of probabilistic models to balance precision and recall according to business needs.\n",
    "\n",
    "Explainability & Interpretability\n",
    "\n",
    "Use SHAP values or LIME to explain individual predictions for actionable business insights.\n",
    "\n",
    "Share insights with marketing teams to guide campaign strategy.\n",
    "\n",
    "Deployment Considerations\n",
    "\n",
    "Wrap the Weighted RF pipeline into a reproducible workflow for real-time or batch predictions.\n",
    "\n",
    "Implement monitoring for data drift and model performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a09ecb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
